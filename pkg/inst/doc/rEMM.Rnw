\documentclass[fleqn, letter, 10pt]{article}
\usepackage[round,longnamesfirst]{natbib}
\usepackage[left=1.5in,top=1.5in,right=1.5in,bottom=1.5in,nohead]{geometry} 
\usepackage{graphicx,keyval,thumbpdf,url}
\usepackage{hyperref}
\usepackage{Sweave}
\SweaveOpts{strip.white=TRUE, eps=FALSE}
\AtBeginDocument{\setkeys{Gin}{width=0.6\textwidth}}


\usepackage[utf8]{inputenc}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsfonts}


\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\sQuote}[1]{`{#1}'}
\newcommand{\dQuote}[1]{``{#1}''}
\newcommand\R{{\mathbb{R}}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\setlength{\parindent}{0mm}
\setlength{\parskip}{3mm plus2mm minus2mm}

%% \VignetteIndexEntry{An Implementation of EMM for R}


\begin{document}

\title{rEMM: An Implementation of EMM for \proglang{R}}
\author{Michael Hahsler and Magaret H. Dunham}
%\date{March 4, 2009}
\maketitle
%\tableofcontents
\sloppy


\abstract{Abstract goes here}

<<echo=FALSE>>=
options(width = 70, prompt="R> ", digits=4)
### for sampling
set.seed(1234)
@


\section{Introduction}
Clustering data streams~\citep{stream_clust:Guha:2000}
has become an important field in recent years. Data streams, large volume data
arriving continuously, are generated by many types of applications including
web click-stream data, computer network monitoring data, telecommunication
connection data, readings from sensor nets, stock quotes, etc. An
important property of data streams for clustering is that data streams contain
massive amounts of data which have to processed in time since
it is impractical to permanently store the data (transient data). 
This leads to the following requirements:

\begin{itemize}
\item Data can only be processed in a single pass or scan of the data
and typically only in the order they arrive.
\item Only a minimal amount of data can be retained and the clusters have to 
be represented in a extremely concise way.
\item Data stream characteristics may change over time 
(e.g., clusters move, merge, disappear or
new clusters may appear).
\end{itemize}

Many algorithms for stream clustering have been proposed.
For example, 
\cite{stream_clust:O'Callaghan:2002}~\citep[see also][]{stream_clust:Guha:2003} 
study the $k$-median 
problem of clustering. Their algorithm
called {\em LOCALSEARCH} divides the data stream into pieces,
clusters each piece individually and then iteratively recluster the resulting
centers to obtain a final clustering.
\cite{stream_clust:Aggarwal:2003} present {\em CluStream} which uses
micro clusters (a temporal extension of cluster feature vectors used
by BIRCH~\citep{stream_clust:Zhang:1996}). 
Micro clusters can be deleted and merged and permanently stored at 
different points in time to allow to create final 
clusterings (recluster micro clusters with $k$-means) for different time frames.
\cite{stream_clust:Kriegel:2003} and \cite{stream_clust:Tasoulis:2007} present
variants of the density based method 
{\em OPTICS}~\citep{stream_clust:Ankerst:1999} suitable for streaming data.
\cite{stream_clust:Aggarwal:2004} introduce {\em HPStream} which finds 
clusters that are well defined in different subsets of the dimensions
of the data. The set of dimensions for each cluster can evolve over time 
and a fading function is used to discount the influence of older data points,
by fading the entire cluster structure.
\cite{stream_clust:Cao:2006} introduce {\em DenStream} which maintains 
micro clusters in real time and uses a variant of 
GDBSCAN~\citep{stream_clust:Sander:1998} to produce a final clustering 
for users.
\cite{stream_clust:Tasoulis:2006} present {\em WSTREAM,} which using 
kernel density estimation to find rectangular windows to represent clusters.
The windows can move, contract, expand and be merged over time. 
%\cite{stream_clust:Aggarwal:2008} Uncertain Data
%\cite{stream_clust:Aggarwal:2009} Massive Data

All approaches center on finding clusters of data points but neglect the
temporal structure of the data which might be crucial to 
understanding the
underlying data. For example, for intrusion detection a user might change from
behavior A to behavior B, both represented by clusters labeled non-suspicious
behavior, but the transition form A to B might be extremely unusual and give
away an intrusion event. 
The Extensible Markov Model \citep[EMM,][]{emm:Dunham:2004}
provides a way to add temporal information in form of an evolving Markov Chain
to data stream clustering algorithms.  Clusters 
correspond to states in the Markov Chain
and transitions representing the temporal information in the data. EMM was
successfully applied 
to rare event and intrusion
detection~\citep{emm:Meng:2006c, emm:Isaksson:2006, emm:Meng:2006a}, web usage
mining~\citep{emm:Lu:2006}, and identifying emerging events and developing
trends~\citep{emm:Meng:2006, emm:Meng:2006b}.  

\marginpar{Discuss related methods (MM, HMM, AMM, ...)}

The paper is organized as follows....

\section{Extensible Markov Model}

EMM can be seen as an evolving Markov Chain (MC) which at each point in time
represents a regular time-homogeneous MC and is updated when new data is
available. In the following we will restrict the discussion to first order MMs
but it is straight forward to extend EMM to higher order 
models~\citep{misc:Kijima:1997}.

A (first order) discreate parameter Markov Chain~\citep{misc:Parzen:1999} is a
special case of a Markov Process in discrete time and with a discrete state
space.
It is characterized by a sequence of random variables
$\{X_t\} = <X_1, X_2, \dots>$ with $t$ being the time index. 
All random variables have the same domain
$\mathrm{dom}(X_i) = S =\{s_1, s_2, \dots, s_k\}$, a countable 
set called the state space. The Markov
property states that the next state is only dependent on the current state.
Formally,

\begin{equation}
P(X_{l+1} = s| X_l =s_l, \dots, X_1=s_1) = P(X_{l+1} = s| X_l =s_l)
\end{equation}

For simplicity we use for transition probabilities the notation 
$a_{ij} = P(X_{l+1} = s_i| X_l =s_j)$ 
where it is appropriate.  Time-homogeneous MC can
be represented by a $k \times k$ transition 
matrix $\mathbf{A}$ containing the transition
probabilities from each state to all other state.

\begin{equation}
\mathbf{A} =
\begin{pmatrix}
a_{11}&a_{12}&\dots&a_{1k}\\
a_{21}&a_{22}&\dots&a_{2k}\\
\vdots&\vdots&\ddots&\vdots\\
a_{k1}&a_{k2}&\dots&a_{kk}\\
\end {pmatrix}
\end{equation}

MCs are a very useful way to keep track of temporal information using the Markov
Property as a relaxation.  With a MC is is easy to forecast the probability of
future states. For example the probability to get from a 
given state to any other
state in $n$ time steps is given by the matrix $\mathbf{A}^n$. With an MC is
also easy to calculate the probability of a new sequence 
of length $l$ as the product of transition probabilities:
\begin{equation}
P(X_l=s_l, X_{l-1}=s_{l-1} \dots, X_1=s_1) = 
\prod_{i=1}^{l-1}{P(X_{i+1} = s_{i+1}| X_{i} =s_i)}
\end{equation}

The probabilities of a Markov Chain can be directly estimated from the 
data using the maximum likelihood method by
\begin{equation}
a_{ij} = c_{ij}/n_i,
\end{equation}
where $c_{ij}$ is the 
observed count of transitions from $s_i$ to $s_j$ in the data
and $n_i=\sum{c_{i.}}$.

Data streams typically contain dimensions with continuous data and/or have
discrete dimensions with a large number of domain
values~\citep{stream_clust:Aggarwal:2009}.  Therefore the data points have to
be mapped onto a manageable number of states first. This mapping is done by
clustering where each cluster (or micro cluster) is represented 
by a state in the MC. The
transition information is maintained during the clustering process by using an
additional data structure representing the MC as a graph.
Since it only uses information (assignment of a data point to a
cluster) which is created by the clustering algorithm anyway, the computational
overhead is minimal.  When the clustering algorithm creates, merges or deletes
clusters, the corresponding state in the MC automatically also gets created,
merged or deleted resulting in an evolving MC, called an EMM.

Many existing clustering algorithms can be easily 
extended to maintain an EMM. For example,
BIRCH~\citep{stream_clust:Zhang:1996},
CluStream~\citep{stream_clust:Aggarwal:2003},
DenStream~\citep{stream_clust:Cao:2006} or
WSTREAM~\citep{stream_clust:Tasoulis:2006} can be used.

In the following we look at the additional data structures and the operations
on these structure which are necessary to maintain an EMM by an existing
clustering algorithm. But first we need to introduce some notation. We define
$S=\{s_1, s_2,\dots,s_k\}$ to be the set of clusters/states 
where $k$ is the current number of clusters.  
We use clusters and state as synonyms since they describe the same
object in the clustering and the MC, respectively.
$\mathbf{n} = (n_1, n_2,\dots,n_k)$ is a vector where $n_i$ is the
number of data points assigned to cluster $s_i$.
The current state $s_c \in \{\epsilon, 1,2,\dots,k\}$ is either
no state~($\epsilon$; before the first data point has arrived) 
or the index of one of the $k$ states.
The transition probabilities from $\epsilon$ to the first 
state are stored in an initial transition probability vector 
$\mathbf{p}$ of length $k$. Typically only one of the elements of $\mathbf{p}$ 
is
one and all others are zero. Hoever, if we have data sets with several 
sequences, $\mathbf{p}$ contains the probability of each state 
to be the first state
in a sequence (see the genetic sequence analysis application in 
Section~\ref{sec:genetic_sequence_analysis}).
%\marginpar{explain $\epsilon$}

\paragraph{Data structures for the EMM.}
Typically a clustering algorithm for stream data uses a very compact
representation for each cluster consisting of a way to summarize the center
and the dispersion of the data as well as how many data points were assigned to
the cluster so far.  Since the cluster represents a state in the EMM we need to
add a data structure to store the outgoing edges and their counts.
For each cluster $s_i$ we need to store a count vector $c_{i.}$.
Together with $n_i$, the number of data points assigned to the cluster, 
it is easy to calculate an estimate of the transition probabilities 
by $a_{i.}=c_{i.}/n_i$.

An way to think of all transition counts in an EMM in 
to look at the a count matrix $\mathbf{C}$ composed 
of all count vectors. It is easy to calculate the estimated
transition probability matrix from the count matrix $\mathbf{C}$ and the 
size vector $n$
\begin{equation}
\mathbf{A} = \mathbf{C} / \mathbf{n}
\end{equation}
A typical EMM is a sparse graph and the count vectors $c_{i.}$ stored with the
clusters contains zeros. Storing the count matrix in sparse data structure 
which allows efficient updating helps to reduce space 
requirements significantly.

For the EMM we keep track of the current state $s_c$ using a single variable
containing a cluster identifier.

\marginpar{Concurrent streams with multiple current states.}

Next we define how the operations typically performed by stream data clustering
algorithms on (micro) clusters can be performed on the EMM.

\paragraph{Adding a data point to an existing cluster.}

When a data point is added to an existing cluster $s_i$, the EMM has to
update the transition count from the current state to the new state by
setting $c_{s_c,i} = c_{s_c,i} +1$.
Finally the current state is set to the new state by $s_c=i$.

\paragraph{Creating a new cluster.}

Whenever the clustering algorithm creates a new (micro) cluster $s_{k+1}$,
the transition count matrix $\mathbf{C}$ has to be enlarged by a row and a 
column. Since the matrix is stored in a sparse data structure, 
this modification incurs only minimal work. 

\paragraph{Deleting clusters.}

When a cluster $s_i$ (typically an outlier cluster) is deleted by 
the clustering algorithm, all we need to do is to remove 
the row $i$ and column $i$ in the transition count matrix $\mathbf{C}$.
This deletes the state.

\paragraph{Merging clusters.}

When two clusters~$s_i$ and $s_j$ are merged into a new cluster $s_m$, 
we need to: 

\begin{enumerate}
\item create new state $s_m$ in $\mathbf{C}$ (see creating cluster above).
\item update the outgoing edges for $s_m$  by $c_{m.} = c_{i.} +c_{j.}$.
\item update the incoming edges for $s_m$ by $c_{.m} = c_{.i} + c_{.j}$.
\item delete columns and rows for the  old states $s_i$ and $s_j$ 
    from $\mathbf{C}$ (see deleting clusters above).
\end{enumerate}

Everything else is typically handled by the clustering algorithm.
It is straight forward to extend the merge to an arbitrary number of clusters
at a time.
Merging states also covers reclustering which is done by many stream clustering
algorithm to create a final clustering for the user/application.


\paragraph{Splitting clusters.}

Splitting micro clusters is typically not implemented in 
data stream clustering algorithms since the individual data points are not 
stored and therefore it is hard to find two new meaningful clusters. 
When clusters are split by algorithms like BIRCH, it typically means that
one or several micro clusters are assigned to a different cluster. This case
does not affect the EMM, since the states are attached to the micro clusters
and thus will move with them to the new cluster.

However, if splitting cluster $s_i$ into two new clusters $s_n$ and $s_m$
is necessary, we create two states with equal
outgoing transition probabilities and a fraction of the incoming
transition probabilities by:
\begin{align*}
c_{n.}& = n_n(c_{i.}/n_i)\\
c_{.n}& = n_n(c_{.i}/n_i)\\
c_{m.}& = n_m(c_{i.}/n_i)\\
c_{.m}& = n_m(c_{.i}/n_i) 
\end{align*}

\paragraph{Fading the cluster structure.}
Clusterings and EMMs adapt to changes in data over time. New data points
influence the clusters and  transition probabilities. However, to enable the
EMM to learn, it has also to forget old data.  Fading the cluster structure is
for example 
used by HPStream~\citep{stream_clust:Aggarwal:2004}.
Fading is achieved by reducing
the weight of old observations in the data stream over time. We use 
a learning rate $\lambda$ to specify the weight over time. The weight
for data that is $t$ timesteps in the past is
\begin{equation}
w_t = 2^{-\lambda t}.
\end{equation}

Since data points are not stored, the weighting has to be done on the 
aggregated observation counts and transition counts. This is easy since
the weight defined above is multiplicative:
\begin{equation}
w_t = \prod_{i=1}^t{2^{-\lambda}}
\end{equation}
\marginpar{better explanation!}
This property allows us to age (multiply) all counts by $2^{-\lambda}$ at 
each time step resulting in compound fading.

These operations cover all cases typically needed to incorporate EMM into 
a clustering algorithm. Next we introduce the simple data stream clustering 
algorithm implemented in \pkg{rEMM}.

\section{Data stream clustering}

Here we use for clustering a simple density-based
clustering algorithm with micro clusters and a reclustering phase.
To represent micro clusters, we use the following 
information:

\begin{itemize}
\item Cluster centers
\item Number of data points assigned to the cluster 
\end{itemize}

%% explain fixed threshold

The cluster centers are either centroids (for Euclidean distance) or pseudo
medoids.  We use medoids since finding canonical centroids in non-Euclidean
space can be a computationally very expensive optimization problem
wich needs access to all data 
points belonging to the cluster~\citep{misc:Leisch:2006}.
However, since we do not store the data points for our clusters,
even exact medoids cannot be found. Therefore, we use fixed pseudo medoids 
which we define as the first data point which creates a 
new cluster. The idea is that since we use a fixed 
threshold around the center, points will be added around the initial data point
which makes it a reasonable center.

Note, that we do not store the sums and sum of squares of observations 
like BIRCH~\citep{stream_clust:Zhang:1996} and similar micro cluster based 
algorithms since this only helps with calculating measures 
meaningful in Euclidean space and the clustering algorithm here is independent
from the chosen proximity measure.

%For the EMM we store a data structure for the transition counts
%and a vector of initial counts 
%One of the clusters is the current cluster.
%\marginpar{Initial probabilities and $\epsilon$}

Algorithm to add a new data point to a clustering:
\begin{enumerate}
\item Computed dissimilarities between the new data point and the
    centers.
\item Find the closest cluster with a dissimilarity smaller than the threshold.
\item If cluster exists then assign the new point to the cluster. 
\item Otherwise create a new cluster for the point.
\end{enumerate}

%After the data point 
%\item Update EMM (the transition count from the current cluster to the new 
%    cluster.
%\item Let the current cluster be the new cluster.

The clustering produces micro clusters. To create a final clustering to present
to a user or to be used by an application, we recluster the micro clusters. For
reclustering an arbitrary algorithm (hierarchical clustering, $k$-means,
$k$-medoids, etc.) can be used. This gives the user the flexibility 
to accommodate
a-priori knowledge about the data and the shape of expected clusters.
For spherical clusters $k$-means or $k$-medoids can be used. 
If clusters of arbitrary shape are expected hierarchical clustering 
with single linkage can be used.

To observe memory limitations, clusters with very low counts (outliers)
can be removed or close clusters can be merged.

\marginpar{Individual threshold for each state}


\section{Implementation details}

Package~\pkg{rEMM} implements the simple clustering algorithm described above 
with an added the temporal EMM layer
using the S3 class system. The package uses the 
infrastructure provided by \pkg{proxy} for dissimilarity computation,
\pkg{cluster} for clustering, \pkg{graph} to represent 
and manipulate the Markov chain as a graph, and \pkg{Rgraphviz} for 
one of visualization options. 

The central class is \class{EMM} which contains
the clustering information as
\begin{itemize}
\item Used dissimilarity measure
\item Dissimilarity threshold for micro clusters
\item An indicator if centroids or pseudo medoids are used
\item Centers. A $k \times d$ matrix containing the centers ($d$-dimensional 
vectors) for the $k$ clusters currently used.
\item The count vector $\mathbf{n}$ with the number of data points
currently assigned to each cluster.
\end{itemize}

and the EMM layer by
\begin{itemize}
\item Markov chain an object of class \class{graphNEL}. The 
directed graph is represened by nodes and an edge 
list (see \pkg{graph}) and represents
the transition count matrix~$\mathbf{C}$ in a sparse format.
\item Current state~$s_c$ as a state index.
\item Initial transition count vector (to represent the initial transition 
probability vector~$\mathbf{p}$)
\end{itemize}

An \class{EMM} object is created by the creator function \func{EMM} which
initializes an empty clustering. To access information about the clustering,
we provide the self explanatory functions 
\func{size}, 
\func{states},
\func{state\_counts} and 
\func{state\_centers}. 


\func{current\_state},
\func{transition},
\func{transition\_matrix},
\func{initial\_transition},


find\_states,

build,
reset,
age,


merge\_states,
remove\_states,
remove\_transitions,
remove\_selftransitions,

recluster\_hclust,
recluster\_kmeans,
recluster\_pam,

rare\_states,
rare\_transitions,
prune


predict\_state,
score,

Class EMM, package graph and complexity.

Clustering, visualization, aging.

\section{Examples}

\subsection{Basic usage}

First, we load the package and a simple data set called {\em EMMTraffic,} which
comes with the package and was used by~\cite{emm:Dunham:2004} to illustrate
EMMs. Each observation in this hypothetical data set is a
vector of seven values obtained from sensors located at specific points on
roads. Each sensor collects a count of the number of vehicles which have
crossed this sensor in the preceding time interval.

<<>>=
library("rEMM")
data(EMMTraffic)
EMMTraffic
@

We use \func{EMM} to create a new EMM object and then build a model
using the EMMTraffic data set. Note that \func{build} takes the whole 
data set at once, but this is only for convenience. Internally the data
points are processed strictly one after the other in one pass.
<<>>=
emm <- EMM(measure="eJaccard", threshold=0.2)
emm <- build(emm, EMMTraffic)
@

The resulting EMM has 4 states.
<<>>=
size(emm)
@

The number of observations represented by each state can be accessed
via \func{state\_counts}.
<<>>=
state_counts(emm)
@

The state centers can be inspected using \func{state\_centers}. 
<<>>=
state_centers(emm)
@

\pkg{rEMM} has several visualization options for EMM models. For example,
as a graph.

<<Traffic_graph, fig=TRUE, include=FALSE>>=
plot(emm, method="graph")
@


The resulting graph is presented in Fig.~\ref{fig:Traffic_graph}. In this 
representation the vertices size and the arrow width code for the number of 
observations represented by each state and the transition probabilities.


\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Traffic_graph}
\caption{An EMM representing the EMMTraffic data set represented as a graph.} 
\label{fig:Traffic_graph}
\end{figure}


The transition probabilities of the EMM model can be calculated using 
\func{transition\_matrix}.
<<>>=
transition_matrix(emm)
@

Alternatively we can get also get the raw transition counts.
<<>>=
transition_matrix(emm, type="counts")
#transition_matrix(emm, type="log_odds")
@

%Log odds are calculated as $ln(a/(1/n))$ where $a$ is the probability of
%the transition and $n$ is the number of states in the EMM.
%$1/n$ is the probability of a transition under the null model
%which assumes that the transition probability from each state
%to each other state (including staying in the same state) is the same, i.e.,
%the null model has a transition matrix with all entries equal to $1/n$.
Individual transition probabilities can be obtained 
more efficiently via \func{transition}.

<<>>=
transition(emm, "1", "2", type="probability")
@


Using the EMM model, we can predict a future state given a current state. 
For example, we can predict the most likely state two time steps away from 
state~2. 
<<>>=
predict_state(emm, n=2, current="2")
@
We can also get the probability distribution of over all states.
<<>>=
predict_state(emm, n=2, current="2", probabilities=TRUE)
@

State~4 was predicted since it has the highest probability. If several states
have the same probability one state is randomly chosen.

\subsection{Manipulating EMMs}
The states of EMMs can be manipulated by removing states or transitions and 
by merging states.
Fig.~\ref{fig:Traffic_man}(a) shows 
again the EMM for the EMMTraffic data set. We can remove a state
with \func{remove\_states}. For example, we remove state~3 and
display the resulting EMM in Fig~\ref{fig:Traffic_man}(b).

<<Traffic_r3, fig=TRUE, include=FALSE>>=
emm_r3 <- remove_states(emm, "3")
plot(emm_r3, "graph")
@

Removing transitions is done with \func{remove\_transitions}.
In the following example we remove the transition from state~5 to state~2
from the original EMM for EMMTraffic.
The resulting graph is shown in Fig~\ref{fig:Traffic_man}(c).

<<Traffic_rt52, fig=TRUE, include=FALSE>>=
emm_rt52 <- remove_transitions(emm, "5", "2")
plot(emm_rt52, "graph")
@

States can be merged using \func{merge\_states}. Here we merge 
states~2 and 5 into a combined state. 
The combined state automatically gets the name of the first state in the 
merge vector. The resulting EMM is shown in Fig~\ref{fig:Traffic_man}(d).
Note that a transition from the combined state (2) to itself is created which 
represents the transition from state~5 to state~2 in the original EMM. 


<<Traffic_m25, fig=TRUE, include=FALSE>>=
emm_m25 <- merge_states(emm, c("2","5"))
plot(emm_m25, "graph")
@

\begin{figure}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_r3}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_rt52}
\\(c)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_m25}
\\(d)
\end{minipage}
\caption{Graph representation for an EMM for the EMMTraffic data set.
(a) shows the original EMM, in (b) state~3 is removed, in 
(c) the transition from state~5 to state~2 is removed, and in
(d) states~2 and 5 are merged.}
\label{fig:Traffic_man}
\end{figure}


\subsection{Using a learning rate and pruning}

EMMs can adapt to changes in data over time. This is achieved by fading 
the cluster structure using a learning rate.
Here we learn an EMM for the EMMTraffic data with a rather high learning
rate of $\lambda=1$. Since the weight is calculated by $w_t = 2^{-lambda t}$,
the observations are weighted $1, \frac{1}{2}, \frac{1}{4},\dots$.

<<Traffic_l, fig=TRUE, include=FALSE>>=
emm_l <- EMM(measure="eJaccard", threshold=0.2, lambda = 1)
emm_l <- build(emm_l, EMMTraffic)
plot(emm_l, "graph")
@

The resulting graph is shown in Fig.~\ref{fig:Traffic_learning}(b).
The states which were created earlier on (states with lower number)
are smaller compared to the original
EMM displayed in Fig.~\ref{fig:Traffic_learning}(a).

Over time states in an EMM can become obsolete and no new observations
are assigned to them. Similarly transitions might become obsolete over time.
To simplify the model and improve efficiency, such obsolete states and
transactions can be pruned. For the example here, we prune all states 
and transitions which have a weighted count of less than $0.1$ and
show the resulting model in Fig.~\ref{fig:Traffic_learning}(c).
Note that with using a learning rate state and transition counts can become
less than one over time.

<<Traffic_lp, fig=TRUE, include=FALSE>>=
emm_lp <- prune(emm_l, count_threshold=0.1)
plot(emm_lp, "graph")
@

A more sophisticated learning scheme is possible by explicitly
calling the function
\func{age} instead of setting $\lambda$ when the EMM is created.
That way the user can specify when a time step occurs, for example, after 
every fifth observation or following real time instead of the stream
of observations.


\begin{figure}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_l}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_lp}
\\(c)
\end{minipage}
\caption{Graph representation for an EMM for the EMMTraffic data set.
(a) shows the original EMM. (b) shows an EMM with a learning rate of
$\lambda=1$. 
(c) EMM with learning rate after pruning with a count threshold of $0.1$.}
\label{fig:Traffic_learning}
\end{figure}

\subsection{Visualization options}

We use a simulated data set called {\em EMMsim} which is included in \pkg{rEMM}.
The data contains four clusters in $\mathbb{R}^2$. Each cluster is
represented by a bivariate normally distributed random variable $X_i
\sim N_2(\mu, \Sigma)$. $\mu$ are the coordinates of the mean of the
distribution and $\Sigma$ is the covariance matrix.
The clusters are well separated.


The temporal structure of the data is modeled by the fixed sequence 
$<1,2,1,3,4>$
through the four clusters
which is 
repeated 40 times (200 data points) for the training data set 
and 5 times (25 data points) for the 
test data.

<<>>=
data("EMMsim")
@

Since the data set is in $2$-dimensional space, 
we can directly visualize the data set as a scatter plot 
(see Fig.~\ref{fig:sim_data}). We also add
the test sequence. To show the temporal structure, the points in the
test data are numbered and lines
connecting the point in sequential order.

<<sim_data, fig=TRUE, include=FALSE>>=
plot(EMMsim_train, col="gray", pch=EMMsim_sequence_train)
lines(EMMsim_test, col ="gray")
points(EMMsim_test, col="red", pch=5)
text(EMMsim_test, labels=1:nrow(EMMsim_test), pos=3)
@

\begin{figure}
\centering
%\begin{minipage}[b]{.49\linewidth}
%\centering
\includegraphics[width=.5\linewidth]{rEMM-sim_data}
%\end{minipage}
\caption{Simulated data with four clusters. The test data is plotted in red
and the temporal structure is depicted by lines between the data points.}%
\label{fig:sim_data}
\end{figure}

We create an EMM by clustering using Euclidean distance and a threshold of
0.1.

<<>>=
emm <- EMM(measure="euclidean", threshold=0.1)
emm <- build(emm, EMMsim_train)
@

<<sim_graph, fig=TRUE, include=FALSE>>=
plot(emm, method="graph")
@

\marginpar{explain state size and arrow width.}
The EMM visualized as a graph is shown in Fig.~\ref{fig:sim_graph}(a).
The position of the vertices of the graph is solely chosen to optimize the
layout which results in a not very informative visualization. 
However, the relative position of the vertices can
be used to represent the dissimilarity between the centers of the states.

<<sim_MDS, fig=TRUE, include=FALSE>>=
plot(emm)
@

This results in the visualization in Fig.~\ref{fig:sim_graph}(b) which shows
the same EMM graph but the relative position of the vertices was determined 
in a way to preserve the dissimilarity information between the centers of the 
states they represent as much as possible.  
A $2$-dimensional layout is computed using multidimensional 
scaling~\citep[MDA][]{misc:Cox:2001}. The size of the states and the width of
the arrows represent again state counts and transition probabilities.

We can also project the points in the data set 
into $2$-dimensional space and then add the centers of the states 
(see Fig.~\ref{fig:sim_graph}(c)).

<<sim_MDS2, fig=TRUE, include=FALSE>>=
plot(emm, data=EMMsim_train)
@


\begin{figure}[tbp]
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_MDS}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_MDS2}
\\(c)
\end{minipage}
\caption{Representation of the EMM for the simulated data. 
(a) As a simple graph. 
(b) A graph using vertex placement to represent dissimilarities.
(c) Projection of state centers onto the simulated data.
}%
\label{fig:sim_graph}
\end{figure}

The simple graph representation in Fig.~\ref{fig:sim_graph}(a) shows a rather
complicated graph for the EMM. However, 
Fig.~\ref{fig:sim_graph}(b) with the vertices positioned
to represent dissimilarities between state centers shows more structure.
The states clearly fall into four groups.
The projection of the state centers onto the data set in
Fig.~\ref{fig:sim_graph}(c) shows that the four groups represent the four
clusters in the data where the larger clusters contain more states.

\subsection{Scoring new sequences}

A score of how likely it is that a sequence was generated by
a given EMM model can be calculated by the normalized product or the sum of the
probabilities on the path along the new sequence of length $l$.
The scores are defined as:

\begin{align}
P_\mathrm{prod} &= \sqrt[l]{\prod_{i=1}^{l-1}{a_{s(i),s(i+1)}}} \\
P_\mathrm{sum} &= \frac{1}{l} \sum_{i=1}^{l-1}{a_{s(i),s(i+1)}}
\end{align}

where $s(i)$ is the state the $i^\textrm{th}$ data point in 
the new sequence is assigned to. The assignment can be done to the 
closest cluster as long as the distance to the center it is smaller
than the threshold. The normalization cancels out the impact of 
$l$ on the score.

As an example, we
calculate how well the some test data fits the EMM created 
for the EMMsim data in the section above. The test data is 
supplied together with the training set in \pkg{rEMM}.
<<>>=
score(emm, EMMsim_test, method="prod", match_state="exact", plus_one=FALSE)
score(emm, EMMsim_test, method="sum", match_state="exact", plus_one=FALSE)
@

\marginpar{introduce log\_odds for comparing EMMs of different size.}

Even though the test data was generated using exactly the same model as 
the training data, 
the normalized product produces a score of 0 and the normalized sum is
also very low. To analyze the problem we can look at the transition
table for the test sequence.

\marginpar{problem with plus\_one and age!}
<<>>=
score(emm, EMMsim_test, match_state="exact", plus_one=FALSE, 
transition_table=TRUE)
@

The low score is caused by 
data points that could not be match to exactly to a cluster
(\code{<NA>} above)
and by missing transitions in the matching 
sequence of states (counts of zero above). 
These missing transitions are the result of the 
fragmentation of the real clusters into many micro clusters
(see Figs.~\ref{fig:sim_graph}(b) and (c)). Suppose 
we have two clusters called cluster A and cluster B and after an 
observation in cluster A always an observation in cluster B follows. If
now cluster A and cluster B are represented by many micro clusters each,
it is very likely that we find a pair of micro clusters (one in A and
one in B) for which we did not see a transition yet.

To reduce the problem of not being able to match a data point to a 
cluster we can use a nearest neighbor approach instead of exact matching
(\code{match\_state="nn"} is the default for \func{score}).
Here a new data point is assigned to the closest cluster even if it falls 
outside the threshold. The problem with missing transactions can be reduced by
starting with a prior distribution of transition probabilities. In the 
most simple case we start with a uniform transition, i.e., if no data is
available we assume that the transitions to all other states are equally 
likely. This can be done by giving each transition an initial count
of one and is implemented as the option \code{plus\_one=TRUE} 
(also the default for \func{score}). It is called plus one since one is added
to the counts at the time when \func{score} executed. It would be 
inefficient to store the ones.

Using nearest neighbor and uniform initial counts of one produce the
following scores.
<<>>=
score(emm, EMMsim_test, method="prod")
@

Since we only have micro clusters, the score is still very small. 
To get a better model, we will recluster the states in the following section.

\subsection{Reclustering states}

For this example, we learn an EMM with a small threshold, and then 
recluster the states of the EMM to find a final clustering.
We use hierarchical clustering with average linkage between state centers.
Then  states in the same cluster are merged to create the clustered
EMM. For the clustering we have to specify
$k$, the number of clusters. To find the optimal number of clusters,
we  create clustered EMMs for 
different values of $k$
and then score
the resulting models using the test data.

<<>>=
## use hierarchical clustering to find best model
emm <- EMM(measure="euclidean", threshold=0.1)
emm <- build(emm, EMMsim_train)
@


We use \func{recluster\_hclust} to create a list of clustered 
EMMs for $k=2,3,\dots,10$ (hierarchical clustering with average link). 
The attribute \code{cluster\_info} contains
information about the clustering including the dendrogram
which is shown in Fig~\ref{fig:sim_hc}.

<<sim_hc, fig=TRUE, include=FALSE>>=
## find best predicting model (clustering)
sq <- 2:10
emmc <- recluster_hclust(emm, k=sq, method ="average") 
plot(attr(emmc, "cluster_info")$dendrogram)
@

\begin{figure}[tbp]
\centering
\includegraphics[width=.5\linewidth]{rEMM-sim_hc}
\caption{Dendrogram for clustering state centers of the EMM build from 
simulated data.}
\label{fig:sim_hc}
\end{figure}

<<>>=
sc <- sapply(emmc, score, EMMsim_test)
names(sc) <- sq
sc
@

The best performing model has a score of \Sexpr{round(max(sc),3)}
and a $k$ of \Sexpr{names(sc[which.max(sc)])}.
This model is depicted in Fig.~\ref{fig:sim_optc}.

<<sim_optc_graph, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(emmc[[which.max(sc)]], method="graph")
@
<<sim_optc_MDS, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(emmc[[which.max(sc)]], data=EMMsim_train)
@

\begin{figure}[tbp]
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-sim_optc_graph}\\
(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{rEMM-sim_optc_MDS}\\
(b)
\end{minipage}
\caption{Best performing final clustering for EMM with a $k$ of 
\Sexpr{names(sc[which.max(sc)])}.}
\label{fig:sim_optc}
\end{figure}

Clustering also finds the original structure with 4 clusters
(see Fig.~\ref{fig:sim_optc}) with all points assigned to the correct
state. 

<<>>=
emmc[[which.max(sc)]]$var_thresholds
@

Compared with the representation in Fig.~\ref{fig:sim_optc}(b) we can see that
states representing more dispersed clusters of points (e.g. state~1) were
assigned larger thresholds than more compact clusters (e.g. state~5).


%Use the Boost Graph Library to check if the found optimal EMMs
%have the same structure
%<<>>=
%library("RBGL")
%isomorphism(emmt[[which.max(st)]]$mm,emmc[[which.max(sc)]]$mm)
%@


\section{Applications}

\subsection{Analyzing river flow data}
The \pkg{rEMM} package also contains a data set called {\em Derwent} which was
originally used by~\cite{emm:Dunham:2004}.  It contains river flow readings
(measured in $m^3$ per second) from six catchments of in the river Derwent and
two of its main tributaries in the northern England.  The data was collected
daily for roughly 5 years from November 1, 1971 to January 31, 1977.  The
catchments are Long Bridge, Matlock Bath, Chat Sworth, What Stand Well, Ashford
(river Wye) and Wind Field Park (river Amber).

The data set is interesting since it contains annual changes of
river levels and also some special flooding events.

<<>>=
data(Derwent)
summary(Derwent)
@

From the summary we see that the average flows vary for the catchments
significantly (from 0.143 to 14.238).  The influence of differences in averages
flows can be removed by scaling the data before building the EMM.  Form the
summary we also see that for the Ashford and Wind Filed Park catchments a
significant amount of observations is not available.  EMM deals with these
missing values by using only the non-missing dimensions of the observations for
the dissimilarity calculations. 

<<Derwent1, fig=TRUE, include=FALSE>>=
plot(Derwent[,1], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[1])
@
\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Derwent1}
\caption{Gauged flows (in $m^3/s$) of the river Derwent at the 
Long Bridge catchment.} 
\label{fig:Derwent1}
\end{figure}

In Fig.~\ref{fig:Derwent1} we can see the annual flow structure for the Long
Bridge catchment with high flows in September to March and lower flows in the
summer months. The first year seams to have more variability in the summer
months and the second year has an unusual event (around the index of 600 in
Fig.~\ref{fig:Derwent1}) with a flow above $100 m^3/s$ which can be classified
flooding.

We build an EMM from the (centered and) scaled river data using Euclidean
distance between the vectors containing the flows from the six catchments and
experimentally found a distance threshold of 3 (just above the
3rd quartile of the distance distribution between all scaled observations) 
to give useful results.

<<Derwent_state_counts, fig=TRUE, include=FALSE, width=10>>=
Derwent_scaled <- scale(Derwent)
emm <- EMM(measure="euclidean", threshold=3)
emm <- build(emm, Derwent_scaled)
#state_counts(emm)
#state_centers(emm)
plot(emm, "state_counts", log="y")
@
\begin{figure}
\centering
\includegraphics[width=.8\linewidth]{rEMM-Derwent_state_counts}
\caption{Distribution of state counts of the EMM for the Derwent data.} 
\label{fig:Derwent_state_counts}
\end{figure}

The resulting EMM has \Sexpr{size(emm)} states. In
Fig.~\ref{fig:Derwent_state_counts} shows that the counts for the states have a
very skewed distribution with states~1 and 2 representing most observations. 

<<Derwent_EMM1, fig=TRUE, include=FALSE>>=
plot(emm)
@
\begin{figure}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{rEMM-Derwent_EMM1}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{rEMM-Derwent_EMM2}
\\(b)
\end{minipage}
\caption{State centers of the EMM for the Derwent data set projected
on $2$-dimensional space.
(a) shows the full EMM and (b) shows a reduced EMM (only the most 
frequently used states)}
\label{fig:Derwent_EMM}
\end{figure}

The projection of the state centers into $2$-dimensional space in
Fig.~\ref{fig:Derwent_EMM}(a) reveals that all states except state~11 and 12
are placed close together and are highly connected. Other states (9, 10, 17 and
19) also seem somewhat different from the majority of states.

Next we look at frequent states. We define rare states here as all 
states that represent less than 0.5\% of the observations. 
On average this translates into less than two daily observation per year.
We calculate a count threshold and then plot the EMM with only the
states which are frequent. For the plot we remove the transitions
from each state to itself to see the most important outgoing transitions
more clearly.

<<Derwent_EMM2, fig=TRUE, include=FALSE>>=
rare_threshold <- sum(state_counts(emm))*0.005
rare_threshold
#plot(prune(emm, rare_threshold, transitions=FALSE))
plot(remove_selftransitions(prune(emm, rare_threshold, transitions=FALSE)))
@

The reduced model depicted in Fig.~\ref{fig:Derwent_EMM}(b) shows that 5 states
represent approximately 99.5\% of the river's behavior.  States~1 and 2 are the
most frequently used states and have a wide arrow (representing transition
probabilities) going both directions between them.
%<<>>=
%rare <- names(which(state_counts(emm)<rare_threshold))
%rare
%@
To analyze the meaning of the two outlier states (11 and 12) 
identified in Fig.~\ref{fig:Derwent_EMM}(a) above, we plot the flows at a 
catchment and mark the observations for these states. 

<<Derwent2, fig=TRUE, include=FALSE>>=
catchment <- 1 
plot(Derwent[,catchment], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[catchment])
state_sequence <- find_states(emm, Derwent_scaled)

mark_states <- function(states, state_sequence, ys, col=0, label=NULL, ...) {
    x <- which(state_sequence %in% states)
    points(x, ys[x], col=col, ...)
    if(!is.null(label)) text(x, ys[x], label, pos=4, col=col)
}

mark_states("11", state_sequence, Derwent[,catchment], col="blue", label="11")
mark_states("12", state_sequence, Derwent[,catchment], col="red", label="12")
#mark_states("9", state_sequence, Derwent[,catchment], col="green", label="9")
#mark_states("3", state_sequence, Derwent[,catchment], col="blue")
@
\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Derwent2}
\caption{Gauged flows (in $m^3/s$) of the river Derwent at the 
Long Bridge catchment.} 
\label{fig:Derwent2}
\end{figure}

In Fig.~\ref{fig:Derwent2} we see that state~12 has a river flow in excess of
$100 m^3/s$ which only happened once in the observation period. The state~11
seems to be a regular observation with medium flow around $20 m^3/s$ and it
needs more analysis to find out why this state is also an outlier directly
leading to state~12.

<<Derwent3, fig=TRUE, include=FALSE>>=
catchment <- 6 
plot(Derwent[,catchment], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[catchment])

mark_states("11", state_sequence, Derwent[,catchment], col="blue", label="11")
mark_states("12", state_sequence, Derwent[,catchment], col="red", label="12")
#mark_states("9", state_sequence, Derwent[,catchment], col="green", label="9")
#mark_states("3", state_sequence, Derwent[,catchment], col="blue")
@
\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Derwent3}
\caption{Gauged flows (in $m^3/s$) of the river Amber at the 
Wind Field Park catchment.} 
\label{fig:Derwent3}
\end{figure}

The catchment at Wind Field Park is at the Amber river which is a tributary of
the Derwent and we see in Fig.~\ref{fig:Derwent3} that the day before the flood
occurs, the flow shoots up to $4 m^3/s$ which is caught by state~11 and gives a
clear sign that a flood is imminent the next day.
 
%Next we look at the probability of going from state~1 (normal condition)
%to state~12 (flooding) in $n=1,2,\dots,10$ days.
%
%<<>>=
%n <- 1:10
%probs <- sapply(n, FUN = function(n) predict_state(emm, n=n, 
%current="1", probabilities=TRUE))[12,]
%names(probs) <- n
%probs
%@
%
%From the probabilities we see that we cannot go directly from 1 to 12.
%Only after two steps we can get to state~12 and the probability is then
%between 0.0005 and 0.0007.
%
%<<fig=TRUE>>=
%hc <- cluster_states(emm)
%plot(hc)
%@
%<<fig=TRUE>>=
% emm20 <- merge_states(emm, cutree(hc, h=20), clustering=TRUE)
% state_centers(emm20)
% plot(remove_selftransitions(emm20))
%@
%<<fig=TRUE>>=
%plot(remove_selftransitions(emm20), "graph")
%@

\subsection{Genetic sequence analysis}
\label{sec:genetic_sequence_analysis}
The \pkg{rEMM} package contains data for 16S ribosomal RNA (rRNA)
sequences for the two phylogenetic classes, Alphaproteobacteria and Mollicutes. 
16S rRNA is a component of the ribosomal subunit 30S and is regularly 
used for phylogenetic studies~\citep[e.g., see][]{rna:Wang:2007}. 
Typically alignment heuristics like BLAST~\citep{rna:Altschul:1990}
or a Hidden Markov Model (HMM)~\citep[e.g.,][]{rna:Hughey:1996} are used
for evaluating the similarity between two or more sequences. However,
these procedures are computationally very expensive. 

An alternative approach is to describe the structure in terms of the occurrence
frequency of so called $n$-words, subsequences of length $n$. Counting the
occurrences of the $4^n$ (there are four bases) $n-words$ is straight forward
and computing similarities between the two frequency profiles if very
efficient. Because no alignment is computed, such methods are called
alignment-free~\citep{rna:Vinga:2003}. 
%A direct application of this
%approach is cd-hit, a method that finds identical subsequences by
%first evaluating the number of matching $n$-word frequencies and only 
%if the 

Here, the sequence data for 30
16S sequences of the phylogenetic class Mollicutes
was preprocessed by cutting the sequences into windows 
of length 100 nucleotides (bases) without overlap and then for each window
the occurrence of triplets of nucleotides was counted resulting in $4^3=64$
counts per window. Each window will be used as an observation
to build the EMM.

\cite{rna:Vinga:2003} review dissimilarity measures used for alignment-free
methods. They most commonly used measures are Euclidean distance, $d^2$ distance
(a weighted Euclidean distance), Mahalanobis distance, Kullback-Leibler
discrepancy (KLD). Since \cite{rna:Wu:2001} find in their experiments that
KLD provides good results while it still can
be computed as fast as Euclidean distance, it is also used here.
Since KLD becomes $-\infty$ for counts of zero, we
add one to all counts which conceptually means that we start building 
the EMM with a prior that all triplets have the equal occurrence 
probability~\cite[see][]{rna:Wu:2001}.
<<>>=
data("16S")

emm <- EMM("Kullback", threshold=0.1)
emm <- build(emm, Mollicutes16S+1)
@

<<Mollicutes_graph, fig=TRUE, include=FALSE>>=
plot(emm, "graph")
## start state for sequences have an initial state probability >0
it <- initial_transition(emm)
it[it>0]
@

\begin{figure}
\centering
\includegraphics[width=\linewidth]{rEMM-Mollicutes_graph}
\caption{An EMM representing 16S sequences from the class Mollicutes 
represented as a graph.} 
\label{fig:Mollicutes_graph}
\end{figure}

The graph representation of the EMM is shown in 
Fig.~\ref{fig:Mollicutes_graph}
Note that each state in the EMM corresponds to one or more windows 
of the rRNA sequence (the size of the state indicates the number 
of windows). The initial transition probabilities show that all sequences start with the first window in states~23, 43, 36, 1 or 47.
Several interesting observations can be made from this 
representation.
\begin{itemize}
\item There exists a path through the graph using only the largest states
    which represents the most common sequence of windows.
\item There are several places the EMM where almost 
    all sequences converge (e.g., 4 and 14)
\item There are places where many possible parallel paths exist 
    (e.g., 7, 27, 20, 35, 33, 28, 65, 71)
\item The windows composition changes over the sequences since there are
    no edges going back or skipping states 
    on the way down.
\end{itemize}

In general it is interesting that the graph has not more loops since 
\cite{rna:Deschavanne:1999} found in their study using Chaos Game 
Representation that the variability along genomes and among genomes is low.
However, they looked at longer sequences and we look here at the micro 
structure of a very short sequence.
These observations merit closer analysis by biologists.

\section{Conclusion}

To come

\bibliographystyle{abbrvnat}
\bibliography{rEMM,rna_sequences,stream_clust,misc}
\end{document}
