\documentclass[fleqn, letter, 10pt]{article}
\usepackage[round,longnamesfirst]{natbib}
\usepackage[left=3cm,top=3cm,right=3cm,bottom=3cm,nohead]{geometry} 
\usepackage{graphicx,keyval,thumbpdf,url}
\usepackage{hyperref}
\usepackage{Sweave}
\SweaveOpts{strip.white=TRUE, eps=FALSE}
\AtBeginDocument{\setkeys{Gin}{width=0.6\textwidth}}


\usepackage[utf8]{inputenc}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsfonts}


\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\sQuote}[1]{`{#1}'}
\newcommand{\dQuote}[1]{``{#1}''}
\newcommand\R{{\mathbb{R}}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\setlength{\parindent}{0mm}
\setlength{\parskip}{3mm plus2mm minus2mm}

%% \VignetteIndexEntry{An Implementation of EMM for R}


\begin{document}

\title{rEMM: An Implementation of EMM for \proglang{R}}
\author{Michael Hahsler and Magaret H. Dunham}
%\date{March 4, 2009}
\maketitle
%\tableofcontents
\sloppy


\abstract{Abstract goes here}

<<echo=FALSE>>=
options(width = 70, prompt="R> ")
### for sampling
set.seed(1234)
@


\section{Introduction}
Short introduction of EMMs~\citep{emm:Dunham:2004}, related methods (MC, HMM,
clustering streams) and discussion of applications.

\section{Implementation details}

Class EMM, package graph and complexity.

\section{Extensions}

clustering, visualization, aging.

\section{Examples}

\subsection{Basic usage}

First, we load the package and a simple data set called {\em EMMTraffic,} which
comes with the package and was used in~\cite{emm:Dunham:2004} to illustrate
EMMs. Each observation in this hypothetical data set is a
vector of seven values obtained from sensors located at specific points on
roads. Each sensor collects a count of the number of vehicles which have
crossed this sensor in the preceding time interval.

<<>>=
library("rEMM")
data(EMMTraffic)
EMMTraffic
@

We use \func{EMM} to create a new EMM object and then build a model
using the EMMTraffic data set.
<<>>=
emm <- EMM(measure="eJaccard", threshold=0.2)
emm <- build(emm, EMMTraffic)
@

The resulting EMM has the following number of states:
<<>>=
size(emm)
@

The number of observations represented by each state can be accessed
via \func{state\_counts}. Note that the counts are only incremented when an 
outgoing transition is created. Therefore state~7 still has a count of 0.
<<>>=
state_counts(emm)
@

The state centers can be inspected using \func{state\_centers}. 
<<>>=
state_centers(emm)
@

\pkg{rEMM} has several visualization options for EMM models. For example,
as a graph.

<<Traffic_graph, fig=TRUE, include=FALSE>>=
plot(emm, method="graph")
@


The resulting graph is presented in Fig.~\ref{fig:Traffic_graph}. In this 
representation the vertices size and the arrow width code for the number of 
observations represented by each state and the transition probabilities.
However, the position of the vertices is chosen to optimize the
layout of the graph. 


\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Traffic_graph}
\caption{An EMM representing the EMMTraffic data set represented as a graph.} 
\label{fig:Traffic_graph}
\end{figure}


The transition probabilities of the EMM model can be calculated using 
\func{transition\_matrix}.
<<>>=
transition_matrix(emm)
@

Alternatively we can get also counts or log odds instead of probabilities.
<<>>=
transition_matrix(emm, type="counts")
transition_matrix(emm, type="log_odds")
@

Log odds are calculated as $ln(a/(1/n))$ where $a$ is the probability of
the transition and $n$ is the number of states in the EMM.
$1/n$ is the probability of a transition under the null model
which assumes that the transition probability from each state
to each other state (including staying in the same state) is the same, i.e.,
the null model has a transition matrix with all entries equal to $1/n$.
Individual transition probabilities can be obtained 
more efficiently via \func{transition}.

<<>>=
transition(emm, "1", "2", type="probability")
@


Using the EMM model, we can predict a future state given a current state. 
From the transition matrix $\mathbf{T}$ we calculate a $\mathbf{T}^n$
which is contains the probability of moving from one state to another
state in $n$ time steps. For
example, from the state we can predict the state in $n=2$ steps. We can also
get the probability distribution of over all states (we predicted the state with
the highest probability).
<<>>=
predict_state(emm, n=2, current="2")
predict_state(emm, n=2, current="2", probabilities=TRUE)
@




\subsection{Manipulating EMMs}
The states of EMMs can be manipulated by removing states or transitions and 
by merging states.
Fig.~\ref{fig:Traffic_man}(a) shows 
again the EMM for the EMMTraffic data set. We can remove a state
with \func{remove\_states}. For example, we remove state~3 and
display the resulting EMM in Fig~\ref{fig:Traffic_man}(b).

<<Traffic_r3, fig=TRUE, include=FALSE>>=
emm_r3 <- remove_states(emm, "3")
plot(emm_r3, "graph")
@

Removing transitions is done with \func{remove\_transitions}.
In the following example we remove the transition from state~5 to state~2.
The resulting graph is shown in Fig~\ref{fig:Traffic_man}(c).

<<Traffic_rt52, fig=TRUE, include=FALSE>>=
emm_rt52 <- remove_transitions(emm, "5", "2")
plot(emm_rt52, "graph")
@

States can be merged using \func{merge\_states}. Here we merge 
states~2 and 5 into a combined state. 
The combined state automatically gets the name of the first state in the 
merge vector. The resulting EMM is shown in Fig~\ref{fig:Traffic_man}(c).
Note that a transition from the combined state (2) is created which 
represents the transition from state~5 to state~2 in the original EMM. 


<<Traffic_m25, fig=TRUE, include=FALSE>>=
emm_m25 <- merge_states(emm, c("2","5"))
plot(emm_m25, "graph")
@

\begin{figure}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_r3}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_rt52}
\\(c)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_m25}
\\(d)
\end{minipage}
\caption{Graph representation for an EMM for the EMMTraffic data set.
(a) shows the original EMM, in (b) state~3 was removed, in 
(c) the transition from state~5 to state~2 was removed, and in
(d) states~2 and 5 are merged.}
\label{fig:Traffic_man}
\end{figure}


\subsection{Using a learning rate and pruning}

EMMs can adapt to changes in data over time. This is achived by reducing
the weight of old observations in the data stream over time. We use 
a learning rate $\lambda$ to specify the weight over time. The weight
for data that is $t$ timesteps in the past is

\begin{equation}
w_t = 2^{-\lambda t}.
\end{equation}

The learning rate is specified when the EMM is created. Every insertion of
a new observation means a new timestep. So the latest added observation
has weight $1$, the observation before $2^{-\lambda}$, etc. Since we do
not store observations but only states and transitions, all the weighing is
incorporated into the counts for states and transitions.

Here we learn an EMM for the EMMTraffic data with a rather high learning
rate of $\lambda=1$ this means that the observations are weighted 
$1, \frac{1}{2}, \frac{1}{4},\dots$.

<<Traffic_l, fig=TRUE, include=FALSE>>=
emm_l <- EMM(measure="eJaccard", threshold=0.2, lambda = 1)
emm_l <- build(emm_l, EMMTraffic)
plot(emm_l, "graph")
@

The resulting graph is shown in Fig.~\ref{fig:Traffic_learning}(b) 
(for comparison Fig.~\ref{fig:Traffic_learning}(a) contains again the original
EMM). 

Over time states in an EMM can become obsolete and no new observations
are assigned to them. Similarily transitions might become obsolete over time.
To simplify the model and improve efficiency, such obsolete states and
transactions can be pruned. For the example here, we prune all states 
and transitions which have a weighted count of less than $0.1$ and
show the resulting model in Fig.~\ref{fig:Traffic_learning}(c).

<<Traffic_lp, fig=TRUE, include=FALSE>>=
emm_lp <- prune(emm_l, count_threshold=0.1)
plot(emm_lp, "graph")
@

With \func{prune} is possible to only prune states or transitions.
A more sophisticated learning scheme is possible by explicitely
calling the function
\func{age} instead of setting $\lambda$ when the EMM is created.
That way the user can specify when a timestep occurs, for example, after 
every fifth observation or following real time instead of the stream
of observations.


\begin{figure}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_l}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_lp}
\\(c)
\end{minipage}
\caption{Graph representation for an EMM for the EMMTraffic data set.
(a) shows the original EMM. (b) shows an EMM with a learning rate of
$\lambda=1$. 
(c) EMM with learning rate after pruning with a count threshold of $0.1$.}
\label{fig:Traffic_learning}
\end{figure}

\subsection{Visualization options}

We use a simulated data set calles {\em EMMsim} which is included in \pkg{rEMM}.
The data contains four clusters in $\mathbb{R}^2$. Each cluster is
represented by a bivariate normally distributed random variable $X_i
\sim N_2(\mu, \Sigma)$. $\mu$ are the coordinates of the mean of the
distribution and $\Sigma$ is the covariance matrix.
The clusters are well separated.


The temporal structure of the data is modeled by the fixed sequence 
$<1,1,1,2,3,2,4>$
through the four clusters
which is 
repeated 20 times for the training data set and 2 times for the 
test data.

<<>>=
data("EMMsim")
@

Since the data set is in $2$-dimensional space, 
we can directly visualize the data set as a scatter plot 
(see Fig.~\ref{fig:sim_data}). We also add
the test sequence. To show the temporal structure, the points in the
test data are numbered and lines
connecting the point in sequential order.

<<sim_data, fig=TRUE, include=FALSE>>=
plot(EMMsim_train, col="gray", pch=EMMsim_sequence_train)
lines(EMMsim_test, col ="gray")
points(EMMsim_test, col="red", pch=5)
text(EMMsim_test, labels=1:nrow(EMMsim_test), pos=3)
@

\begin{figure}
\centering
%\begin{minipage}[b]{.49\linewidth}
%\centering
\includegraphics[width=.5\linewidth]{rEMM-sim_data}
%\end{minipage}
\caption{Simulated data with four clusters. The test data is plotted in red
and the temporal structure is depicted by lines between the data points.}%
\label{fig:sim_data}
\end{figure}

<<>>=
emm <- EMM(measure="euclidean", threshold=0.1)
emm <- build(emm, EMMsim_train)
@

We can visualize the resulting EMM as a graph.
<<sim_graph, fig=TRUE, include=FALSE>>=
plot(emm, method="graph")
@


The position of the vertices of the graph is solely chosen to optimize the
layout (see Fig.~\ref{fig:sim_graph}(a)). 
However, the position of the vertices can
be used to represent the dissimilarity between the centers of the states they
represent.

<<sim_MDS, fig=TRUE, include=FALSE>>=
plot(emm)
@

This results in the visualization in Fig.~\ref{fig:sim_graph}(b) which shows
the same EMM graph but the position of the vertices was determined in a way to
preserve the dissimilarity information between the centers of the states they
represent as much as possible.  For data with higher dimensionality, a
$2$-dimensional layout is computed using multidimensional scaling (MDS).

We can also project the points in the data set 
into $2$-dimensional space and then add the centers of the states 
(see Fig.~\ref{fig:sim_graph}(c)).

<<sim_MDS2, fig=TRUE, include=FALSE>>=
plot(emm, data=EMMsim_train)
@


\begin{figure}[tbp]
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_MDS}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_MDS2}
\\(c)
\end{minipage}
\caption{Representation of the EMM for the simulated data. 
(a) As a simple graph. 
(b) A graph using vertex placement to represent dissimilarities.
(c) Projection of state centers onto the simulated data.
}%
\label{fig:sim_graph}
\end{figure}

The simple graph representation in Fig.~\ref{fig:sim_graph}(a) shows a rather
complicated graph for the EMM. However, 
Fig.~\ref{fig:sim_graph}(b) with the vertices positioned
to represent dissimilarities between state centers shows more structure.
The states clearly fall into four groups.
The projection of the state centers onto the data set in
Fig.~\ref{fig:sim_graph}(c) shows that the four groups represent the four
clusters in the data where the larger clusters contain more states.

\subsection{Scoring new sequences}

A scores of how likely it is that a sequence was generated by
a given EMM model can be calculated by the normalized product or the sum of the
probabilities on the path along the new sequence. For this example, we
calculate how well the test data fits the EMM.
<<>>=
score(emm, EMMsim_test, method="prod")
score(emm, EMMsim_test, method="sum")
@

Even though the test data is generated using exactly the same model as 
the training data, 
the normalized product produces a score of 0 and the normalized sum is
also very low. To analyze the problem we can look at the transition
table for the test sequence.

<<>>=
score(emm, EMMsim_test, transition_table=TRUE)
@

The low score is caused by missing transitions in the matching 
sequence of states. These missing transitions are the result of the 
fragmentation of the real clusters into many micro-clusters
(see Figs.~\ref{fig:sim_graph}(b) and (c)). Suppose 
we have two clusters called cluster A and cluster B and after an 
observation in cluster A always an observation in cluster B follows. If
now cluster A and cluster B are represented by many micro clusters each,
the chances are high that we find a pair of micro clusters (one in A and
one in B) for which we did not see a transition.
\marginpar{start with count 1 or prior prob distr.}

\subsection{Finding the optimal threshold}

We can create multiple EMMs using different thresholds and calculate the score
on the test data for each model. The optimal threshold is then the threshold
which produces the highest score on the test data.

We use again the simulated data from above for the following example.
First we create EMMs for all thresholds 
from $0.2$ to $0.7$ in $0.05$ increments using the training data.

<<>>=
## find best predicting model (threshold)
sq <- seq(0.2,0.7, by=0.05)
emmt <- lapply(sq, FUN=function(t) {
        emm <- EMM(measure="euclidean", threshold=t)
        build(emm, EMMsim_train)
    })
names(emmt) <- sq
@

The number of states of the models is decreasing with an increasing 
dissimilarity threshold.
<<>>=
sapply(emmt, size)
@

Next we score the test data against all models.
<<>>=
st <- sapply(emmt, score, EMMsim_test)
st
@

The best performing model has a score of \Sexpr{round(max(st),3)}
and uses a threshold of \Sexpr{names(st[which.max(st)])}.
This model is depicted in Fig.~\ref{fig:sim_optt}.
It almost perfectly finds the original structure with 4 clusters. 
Only one point between states~1 and 4 is assigned to state~4 instead
of state~1 (see Fig.~\ref{fig:sim_optt}(b)).
This produces in the EMM a transition from state~1 to state~4 
(see Fig.~\ref{fig:sim_optt}(a))
which was not in the original data generation model.

<<sim_optt_graph, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(emmt[[which.max(st)]], method="graph")
@
<<sim_optt_MDS, fig=TRUE, include=FALSE, echo = FALSE>>=
plot(emmt[[which.max(st)]], data=EMMsim_train)
@

\begin{figure}[tbp]
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_optt_graph}\\
(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{rEMM-sim_optt_MDS}\\
(b)
\end{minipage}
\caption{Best performaing EMM with a threshold of 
\Sexpr{names(st[which.max(st)])}}
\label{fig:sim_optt}
\end{figure}


\subsection{Cluster states}

For this example, we learn an EMM with a small threshold, and then 
find a clustering for the states of the resulting EMM.
We use hierarchical clustering with average linkage between state centers.
Then  states in the same cluster are merged to create the clustered
EMM. For the clustering we have to specify
$k$, the number of clusters. To find the optimal number of clusters,
we  create clustered EMMs for 
different values of $k$
and then score
the resulting models using the test data.

<<>>=
## use hierarchical clustering to find best model
emm <- EMM(measure="euclidean", threshold=0.1)
emm <- build(emm, EMMsim_train)
@

<<sim_hc, fig=TRUE, include=FALSE>>=
## find best predicting model (clustering)
hc <- cluster_states(emm)
plot(hc)
@

\begin{figure}[tbp]
\centering
\includegraphics[width=.5\linewidth]{rEMM-sim_hc}
\caption{Dendrogram for clustering state centers of the EMM build from 
simulated data.}
\label{fig:sim_hc}
\end{figure}

\func{cluster\_states} performs hierarchical clustering on the state centers
and returns the cluster structure as a dendrogram. The resulting
dendrogram is shown in Fig~\ref{fig:sim_hc}.

Next we create a list of clustered EMMs for $k=3,4,\dots,10$ and
score each model against the test data.
<<>>=
sq <- 3:10
emmc <- lapply(sq, FUN=function(k) {
        cl <- cutree(hc, k=k)
        merge_states(emm, cl, clustering=TRUE)
    })
names(emmc) <- sq

sc <- sapply(emmc, score, EMMsim_test)
sc
@

The best performing model has a score of \Sexpr{round(max(sc),3)}
and a $k$ of \Sexpr{names(sc[which.max(sc)])}.
This model is depicted in Fig.~\ref{fig:sim_optc}.

<<sim_optc_graph, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(emmc[[which.max(sc)]], method="graph")
@
<<sim_optc_MDS, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(emmc[[which.max(sc)]], data=EMMsim_train)
@

\begin{figure}[tbp]
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_optc_graph}\\
(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{rEMM-sim_optc_MDS}\\
(b)
\end{minipage}
\caption{Best performaing EMM with a $k$ of 
\Sexpr{names(sc[which.max(sc)])}.}
\label{fig:sim_optc}
\end{figure}

\marginpar{seems not to work perfectly}
Note that clustering perfectly finds the original structure with 4 clusters
(see Fig.~\ref{fig:sim_optc}) with all points assigned to the correct
state. The reason why the point between states~1 and 4 can be assigned
correctly is that during merging states, an individual new threshold is 
calculated for each state. 
The obtained thresholds for the example are:

<<>>=
emmc[[which.max(sc)]]$var_thresholds
@

Compared with the representation in Fig.~\ref{fig:sim_optc}(b)
we can states representing larger clusters of points also were 
assigned a larger threshold.


%Use the Boost Graph Library to check if the found optimal EMMs
%have the same structure
%<<>>=
%library("RBGL")
%isomorphism(emmt[[which.max(st)]]$mm,emmc[[which.max(sc)]]$mm)
%@

\subsection{Flooding data}
The \pkg{rEMM} package also contains a data set
called {\em Derwent} which was originally used by~\cite{emm:Dunham:2004}.
It contains river flow readings (measured in $m^3$ per second) 
from six catchments of in the
river Derwent and two of its main tributaries
in the northern England. 
The data was collected daily for roughly 5 years from November 1, 1971 to 
January 31, 1977. 
The catchments are Long Bridge, Matlock Bath, Chat Sworth, 
What Stand Well, Ashford (river Wye) and Wind Field Park (river Amber).

The data set is interesting since it cointains annual changes of
river levels and also some special flooding events.

<<>>=
data(Derwent)
summary(Derwent)
@

From the summary we see that the average flows vary for the catchments
significantly (from 0.143 to 14.238).  The influence of differences in averages
flows can be removed by scaling the data before building the EMM.  Form the
summary we also see that for the Ashford and Wind Filed Park catchments a
significant amount of observations is not available.  EMM deals with these
missing values by using only the non-missing dimensions of the observations for
the dissimilarity calculations. 

<<fig=TRUE>>=
plot(Derwent[,1], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[1])
@

In Fig.... we can see the annual flow structure
for the Long Bridge catchment
with high flows in September to
March and lower flows in the summer months. The first year seams to have more
variability in the summer months and the second year has an unusual event
(around the index of 600 in Fig.....) 
with a flow above $100 m^3/s$ which can be classified flooding.

We build an EMM from the (centered and) scaled river data using Euclidean
distance between the vectors containing the flows from the six cachements and
experimentally found a distance threshold of 3 (just above the
3rd qartile of the distance distribution between all scaled observations) 
to give useful results.

<<fig=TRUE>>=
Derwent_scaled <- scale(Derwent)
emm <- EMM(measure="euclidean", threshold=3)
emm <- build(emm, Derwent_scaled)
#state_counts(emm)
#state_centers(emm)
plot(emm, "state_counts", log="y")
@

The resulting EMM has \Sexpr{size(emm)} states. In Fig.... shows 
that the counts for the states have a very skewed distribution 
with states~1 and 2 representing
most observations. 

<<fig=TRUE>>=
plot(emm)
@

The projection of the state centers into $2$-dimensional space reveals
that all states except state~11 and 12 are placed close together and 
are highly connected. Other states (9, 10, 17 and 19) also seem somewhat 
different from the majority of states.


Next we look at frequent states. We define rare states here as all 
states that represent less than 0.5\% of the observations. 
On average this translates into less than two daily observation per year.
We calculate a count threshold and then plot the EMM with only the
states which are frequent. For the plot we remove the transitions
from each state to itself to see the most important outgoing transitions
more clearly.
<<fig=TRUE>>=
rare_threshold <- sum(state_counts(emm))*0.005
rare_threshold
#plot(prune(emm, rare_threshold, transitions=FALSE))
plot(remove_selftransitions(prune(emm, rare_threshold, transitions=FALSE)))
@

The model shows that 5 states represent approximately 99.5\% of the river's
behavior.  States~1 and 2 are the most frequently used states and have a wide
arrow (representing transition probabilities) going both directions between
them.
%<<>>=
%rare <- names(which(state_counts(emm)<rare_threshold))
%rare
%@
To analyze the meaning of the two outlier states (11 and 12) 
identified in Fig...... above, we plot the flows at a 
catchment and mark the observations for these states. 

<<fig=TRUE>>=
catchment <- 1 
plot(Derwent[,catchment], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[catchment])
state_sequence <- find_states(emm, Derwent_scaled)

mark_states <- function(states, state_sequence, ys, col=0, label=NULL, ...) {
    x <- which(state_sequence %in% states)
    points(x, ys[x], col=col, ...)
    if(!is.null(label)) text(x, ys[x], label, pos=4, col=col)
}

mark_states("11", state_sequence, Derwent[,catchment], col="blue", label="11")
mark_states("12", state_sequence, Derwent[,catchment], col="red", label="12")
#mark_states("9", state_sequence, Derwent[,catchment], col="green", label="9")
#mark_states("3", state_sequence, Derwent[,catchment], col="blue")
@

In Fig.... we see that state~12 has a river flow in eccess of $100 m^3/s$ which only happend once 
in the observation period. The state~11 seems to be a regular observation
with medium
flow around $20 m^3/s$ and it needs more analysis to find out why this 
state is also an outlier directly leading to state~12.

<<fig=TRUE>>=
catchment <- 6 
plot(Derwent[,catchment], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[catchment])

mark_states("11", state_sequence, Derwent[,catchment], col="blue", label="11")
mark_states("12", state_sequence, Derwent[,catchment], col="red", label="12")
#mark_states("9", state_sequence, Derwent[,catchment], col="green", label="9")
#mark_states("3", state_sequence, Derwent[,catchment], col="blue")
@

The cachment at Wind Field Park is at the Amber river which is a tributary of
the Derwent and we see that a day before the flood occurs, the flow shoots
up to $4 m^3/s$ which is cought by state~11 and gives a clear sign that 
a flood is iminent the next day.
 



Next we look at the probability of going from state~1 (normal condition)
to state~12 (flooding) in $n=1,2,\dots,10$ days.

<<>>=
n <- 1:10
probs <- sapply(n, FUN = function(n) predict_state(emm, n=n, 
current="1", probabilities=TRUE))[12,]
names(probs) <- n
probs
@

From the probabilities we see that we cannot go directly from 1 to 12.
Only after two steps we can get to state~12 and the probability is then
between 0.0005 and 0.0007.

%<<fig=TRUE>>=
%hc <- cluster_states(emm)
%plot(hc)
%@
%<<fig=TRUE>>=
% emm20 <- merge_states(emm, cutree(hc, h=20), clustering=TRUE)
% state_centers(emm20)
% plot(remove_selftransitions(emm20))
%@
%<<fig=TRUE>>=
%plot(remove_selftransitions(emm20), "graph")
%@

\subsection{rRNA sequence}
The \pkg{rEMM} package contains data for 16S ribosomal RNA (rRNA)
sequences for the two phylogenetic classes, Alphaproteobacteria and Mollicutes. 
16S rRNA is a component of the ribosomal subunit 30S and is regularly 
used for phylogenetic studies (ref .......). 

The sequence data was preprocessed by cutting the sequences into windows 
of length 100 nucleodites (bases) without overlap and then for each window
the occurrence of triplets of nucleodites was counted resulting in $4^3=64$
counts per window. 

Idea: Structural information (Ref:.....)

The windows are used as observations to build an EMM.
Here we look at the 16S sequence of Mollicutes. As the dissimilarity
measure we use Kullback-Leibler (KL) divergence which was identified
in studies (REF....) as on of the best performing measure for count data
from genetic sequences. Since KL becomes $-\infty$ for counts of zero, we
add one to all counts which means that we start building the EMM with a
prior that all triplets have the equal occurence probability.
<<>>=
data("16S")

emm <- EMM("Kullback", threshold=0.1)
emm <- build(emm, Mollicutes16S+1)
@

<<fig=TRUE>>=
plot(emm, "graph")

## start state for sequences have an initial state probability >0
it <- initial_transition(emm)
it[it>0]
@

The graph representation of the EMM is shown in Fig..... 
Note that each state in the EMM corresponds to one or more windows 
of the rRNA sequence (the size of the state indicates the number 
of windows). The initial transition probabilities show that all sequences start with the first window in state~1
Several interesting observations can be made from this 
representation.
\begin{itemize}
\item There exists a path through the graph using only the largest states
    which represents the most common sequence of windows.
\item There are several places the EMM where all sequences converge (e.g., 14)
\item There are places where many possible parallel paths exist 
    (e.g., 32, 35, 7, 19, 27,45)
\item The windows composition changes over the sequences since there are
    only a few edges going back (e.g., from 24 to 2) or skiping states 
    on the way down (e.g., from 2 to 11).
\end{itemize}

These observations merit closer analysis by biologists.

\section{Conclusion}

To come

\bibliographystyle{abbrvnat}
\bibliography{rEMM}
\end{document}
